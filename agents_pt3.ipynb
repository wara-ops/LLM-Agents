{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2b0b62",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Agents Demystified - part 3\n",
    "\n",
    "In this installment we'll let our simple agent loose on portal data by revisiting the introductory tutorial on [using the portal and Pandas](../PortalAndPandas/introduction.ipynb). In doing so, we'll also hit some of the limits of our design. \n",
    "\n",
    "First we will use the agent on log data from ERDC and run some examples, becoming aware of the limitations of our design choices in this tutorial, before concluding with a summary of the main takeaways and a discussion of what would be required components of a \"real\" agentic system. Hopefully, by then, agents in the wild will make a lot more sense as you have enough experience to push further by yourself.\n",
    "\n",
    "### Getting the DataportalClient\n",
    "\n",
    "First we need to get the DataportalClient module, and then set up the portal token but this time we'll pass it in the environment rather than pasting it into a script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e797220",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/wara-ops/DataportalClient.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\" # <-- Paste your portal token between the quotes\n",
    "\n",
    "import os\n",
    "os.environ['DATAPORTAL_CLIENT_TOKEN'] = token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc6693",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Getting the agent and LLM-as-a-Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca23804",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ollama tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21acfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.agents2 import Agent\n",
    "from helpers.agents3 import execute_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b291c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host and model definitions\n",
    "OLLAMA_HOST = 'http://10.129.20.4:9090'\n",
    "OLLAMA_MODEL = 'gemma3:27b-it-qat'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c78dd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Sanity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(OLLAMA_HOST, OLLAMA_MODEL, tools=[execute_script])\n",
    "print(agent.task(\"What time is it?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379ba70",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Accessing portal data\n",
    "\n",
    "This is where we hit the first limitation in the current design. Since our agent relies on `execute_script` (which runs a separate process) to evaluate any generated code we really can't keep any state (even in the agent's internal loop) except what is contained in the (text) message history and, possibly more limiting, any (portal related) code that needs to be used by scripts has to be _imported_ by the script as we can't simply pass python objects to `execute_script`.   \n",
    "\n",
    "Thus, to simplify things we'll place a portal \"helper\" function the working director that can be imported by the script. Running the cell below will write the python code to `xerces.py` in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\"\"\"\n",
    "cat << EOF > work/xerces.py\n",
    "from dataportal import DataportalClient\n",
    "import pandas\n",
    "\n",
    "\n",
    "A simple helper function to access a (hard-coded) log from the portal\n",
    "\n",
    "def get_log() -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a Xerces log file as a pandas DataFrame.\n",
    "    Xerces is a cloud service, built using OpenStack, consisting of subsystem modules. \n",
    "    The log entries are tagged with the name of the emitting module.\n",
    "\n",
    "    The modules are:\n",
    "    - 'Nova' handles virtual machines.\n",
    "    - 'Neutron' provides \"networking-as-a-service\".\n",
    "    - 'Swift' provides an object storage.\n",
    "    - 'Cinder' offers persistent block storage.\n",
    "    - 'Horizon' is the GUI for OpenStack.\n",
    "    - 'Keystone' provides identity services.\n",
    "    - 'Glance' handle discovery and retrieve of virtual machine images.\n",
    "    - 'Heat' orchestrates multiple composite cloud applications.\n",
    "\n",
    "    Args:\n",
    "        None \n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: the Xerces log file as a pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    client = DataportalClient()\n",
    "    client.fromDataset('ERDClogs-parsed')\n",
    "    return client.getData(fileID=36666)\n",
    "    \n",
    "EOF\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1211416",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Analyzing the logs\n",
    "\n",
    "Now we're ready to go! The first thing is to inform the agent how to access the log data, and get some feedback into the message history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(OLLAMA_HOST, OLLAMA_MODEL, tools=[execute_script])\n",
    "print(agent.task(\"You have access to logs from the cloud service 'Xerces' by importing the function 'get_log' from 'xerces.py' in your working directory. After importing it, you can retrieve documentation for the function using 'help(get_log)'. Give me a terse summary of the 'Xerces' service subsystems.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(agent.message_history())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d7aa1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "Now we can e.g. ask questions on what was logged …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee4f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.task(\"Get info on the log dataframe and list the column labels.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e600a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "… and quickly scan the log for signs of trouble …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7179e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.task(\"Great! Get all 'log_level' entries and make a list of all types. Only list each type once.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28641f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.task(\"The naming appears inconsistent. Coalesce 'log_level' types as the uppercased long form, e.g. 'warn', 'warning', 'Warning' should all be 'WARNING'. Get all 'log_level' entries and make a list of all types.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d9dcd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Hitting another limit\n",
    "\n",
    "About this far into the conversation things usually start to get shaky. By now the message history is quite long (the system prompt plus all internal conversations plus our queries) and it is fed to the LLM _on every turn_ which means we are taxing the LLM's context window as well as the physical memory required by the LLM to process such an extensive string of tokens. Every answer tend to take longer and longer to complete. Furthermore, if the message history grows beyond the allowed context window size, the input is simply truncated and the agent is not likely to respond sensibly.\n",
    "\n",
    "To see the message history (sans system prompt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53334a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.message_history())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451cfec",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Band aid…\n",
    "Starting a new agent solves the problem with the context size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(OLLAMA_HOST, OLLAMA_MODEL, tools=[execute_script])\n",
    "instructions = \"\"\"\n",
    "You have access to logs from the cloud service 'Xerces' by importing the function 'get_log' from 'xerces.py' in your working directory. \n",
    "After importing it, you can retrieve documentation for the function using 'help(get_log)'. \n",
    "Unify 'log_level' types as the uppercased long form, e.g. 'warn', 'warning', 'Warning' should all be 'WARNING'.\n",
    "Modify the dataframe accordingly and save it in CSV format as 'cleaned_log.csv'\n",
    "\"\"\"\n",
    "print(agent.task(instructions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffa28e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "You can open the resulting file ['work/cleaned_log.csv'](work/cleaned_log.csv) and verify the fixes.\n",
    "\n",
    "### Visualizing log events\n",
    "\n",
    "Just as in the previous part, the agent is capable of visualizing data as per your request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(OLLAMA_HOST, OLLAMA_MODEL, tools=[execute_script])\n",
    "instructions = \"\"\"\n",
    "You have access to logs from the cloud service 'Xerces' by importing the function 'get_log' from 'xerces.py' in your working directory. \n",
    "After importing it, you can retrieve documentation for the function using 'help(get_log)'. \n",
    "Display a bar chart over the number of log entries per module ('Logger')\n",
    "\"\"\"\n",
    "\n",
    "print(agent.task(instructions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17774d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continued\n",
    "print(agent.task(\"Using the log dataframe, display a bar chart over the number of log entries per type ('log_level') for entries whose 'Logger' column is tagged 'openstack.neutron'.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36084c57",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Summary and dicussion\n",
    "\n",
    "So, in this three-part tutorial we have tried to show that AI agents are not magical by building a simple, yet capable, agent around a vanilla LLM using nothing but plain python.\n",
    "\n",
    "As always, the devil is in the details, and the simple design hit a number of limitations. The important take-away is that those limitations were not due to the core concept, but necessary to keep this tutorial short and to the point. \n",
    "\n",
    "Finally, an example of how a full implementation of an AI agent could be structured (hopefully this picture makes sense after following the tuorials):\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    user_prompt(\"User prompt\"):::start_stop\n",
    "    agent((\"Agent\")):::model\n",
    "    llm[\"LLM\"]\n",
    "    hist[\"Curated history\"]\n",
    "    long[\"Long term memory\"]\n",
    "    short[\"Short term memory\"]\n",
    "    rag[\"RAG\"]\n",
    "    tools[\"Tools\"]\n",
    "    work_area[\"Work area\"]\n",
    "    state[\"State\"]\n",
    "    \n",
    "\n",
    "    user_prompt --> agent --> llm\n",
    "    llm --> parsing --> agent\n",
    "\n",
    "    subgraph Conversation History\n",
    "        hist <--> long\n",
    "        hist <--> short\n",
    "    end\n",
    "    agent <--> hist\n",
    "\n",
    "    rag <--> agent\n",
    "\n",
    "    subgraph Tooling\n",
    "        tools <--> work_area <--> state\n",
    "    end\n",
    "    agent --> tools\n",
    "    state --> agent\n",
    "\n",
    "    classDef start_stop fill: #9f6, stroke: #333, stroke-width:2px;\n",
    "    classDef model fill: #f96, stroke: #333, stroke-width:1px;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423439c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
